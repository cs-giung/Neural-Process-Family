# celeba
# python scripts/jax/train.py -s 142 -f configs/image/celeba/cnp.yaml  -lr 0.002811706625952 --datasets.train.batch_size 128 --train.num_epochs 10
# python scripts/jax/train.py -s 142 -f configs/image/celeba/np.yaml   -lr 0.002811706625952 --datasets.train.batch_size 128 --train.num_epochs 10 --model.kwargs.loss_type ml
# python scripts/jax/train.py -s 142 -f configs/image/celeba/bnp.yaml  -lr 0.002811706625952 --datasets.train.batch_size 128 --train.num_epochs 10
# python scripts/jax/train.py -s 142 -f configs/image/celeba/canp.yaml -lr 0.001581138830084 --datasets.train.batch_size 128 --train.num_epochs 10
# python scripts/jax/train.py -s 142 -f configs/image/celeba/anp.yaml  -lr 0.001581138830084 --datasets.train.batch_size 128 --train.num_epochs 10 --model.kwargs.loss_type ml
# python scripts/jax/train.py -s 142 -f configs/image/celeba/banp.yaml -lr 0.001581138830084 --datasets.train.batch_size 128 --train.num_epochs 10

# svhn
# python scripts/jax/train.py -s 142 -f configs/image/svhn/cnp.yaml    -lr 0.005000000000000 --datasets.train.batch_size 128 --train.num_epochs 10
# python scripts/jax/train.py -s 142 -f configs/image/svhn/np.yaml     -lr 0.002811706625952 --datasets.train.batch_size 128 --train.num_epochs 10 --model.kwargs.loss_type ml
# python scripts/jax/train.py -s 142 -f configs/image/svhn/bnp.yaml    -lr 0.005000000000000 --datasets.train.batch_size 128 --train.num_epochs 10
# python scripts/jax/train.py -s 142 -f configs/image/svhn/canp.yaml   -lr 0.001581138830084 --datasets.train.batch_size 128 --train.num_epochs 10
# python scripts/jax/train.py -s 142 -f configs/image/svhn/anp.yaml    -lr 0.000889139705019 --datasets.train.batch_size 128 --train.num_epochs 10 --model.kwargs.loss_type ml
# python scripts/jax/train.py -s 142 -f configs/image/svhn/banp.yaml   -lr 0.002811706625952 --datasets.train.batch_size 128 --train.num_epochs 10

# mnist
python scripts/jax/train.py -s 142 -f configs/image/mnist/cnp.yaml   -lr 0.002811706625952 --datasets.train.batch_size 128 --train.num_epochs 10
python scripts/jax/train.py -s 142 -f configs/image/mnist/np.yaml    -lr 0.002811706625952 --datasets.train.batch_size 128 --train.num_epochs 10 --model.kwargs.loss_type ml
python scripts/jax/train.py -s 142 -f configs/image/mnist/bnp.yaml   -lr 0.002811706625952 --datasets.train.batch_size 128 --train.num_epochs 10
python scripts/jax/train.py -s 142 -f configs/image/mnist/canp.yaml  -lr 0.002811706625952 --datasets.train.batch_size 128 --train.num_epochs 10
python scripts/jax/train.py -s 142 -f configs/image/mnist/anp.yaml   -lr 0.000500000000000 --datasets.train.batch_size 128 --train.num_epochs 10 --model.kwargs.loss_type ml
python scripts/jax/train.py -s 142 -f configs/image/mnist/banp.yaml  -lr 0.000889139705019 --datasets.train.batch_size 128 --train.num_epochs 10



# celeba
python scripts/jax/train.py -s 142 -f configs/image/celeba/cnp.yaml  -lr 0.002811706625952 --datasets.train.batch_size 128 --train.num_epochs 20
python scripts/jax/train.py -s 142 -f configs/image/celeba/np.yaml   -lr 0.002811706625952 --datasets.train.batch_size 128 --train.num_epochs 20 --model.kwargs.loss_type ml
python scripts/jax/train.py -s 142 -f configs/image/celeba/bnp.yaml  -lr 0.002811706625952 --datasets.train.batch_size 128 --train.num_epochs 20
python scripts/jax/train.py -s 142 -f configs/image/celeba/canp.yaml -lr 0.001581138830084 --datasets.train.batch_size 128 --train.num_epochs 20
python scripts/jax/train.py -s 142 -f configs/image/celeba/anp.yaml  -lr 0.001581138830084 --datasets.train.batch_size 128 --train.num_epochs 20 --model.kwargs.loss_type ml
python scripts/jax/train.py -s 142 -f configs/image/celeba/banp.yaml -lr 0.001581138830084 --datasets.train.batch_size 128 --train.num_epochs 20

# svhn
python scripts/jax/train.py -s 142 -f configs/image/svhn/cnp.yaml    -lr 0.005000000000000 --datasets.train.batch_size 128 --train.num_epochs 20
python scripts/jax/train.py -s 142 -f configs/image/svhn/np.yaml     -lr 0.002811706625952 --datasets.train.batch_size 128 --train.num_epochs 20 --model.kwargs.loss_type ml
python scripts/jax/train.py -s 142 -f configs/image/svhn/bnp.yaml    -lr 0.005000000000000 --datasets.train.batch_size 128 --train.num_epochs 20
python scripts/jax/train.py -s 142 -f configs/image/svhn/canp.yaml   -lr 0.001581138830084 --datasets.train.batch_size 128 --train.num_epochs 20
python scripts/jax/train.py -s 142 -f configs/image/svhn/anp.yaml    -lr 0.000889139705019 --datasets.train.batch_size 128 --train.num_epochs 20 --model.kwargs.loss_type ml
python scripts/jax/train.py -s 142 -f configs/image/svhn/banp.yaml   -lr 0.002811706625952 --datasets.train.batch_size 128 --train.num_epochs 20

# mnist
python scripts/jax/train.py -s 142 -f configs/image/mnist/cnp.yaml   -lr 0.002811706625952 --datasets.train.batch_size 128 --train.num_epochs 20
python scripts/jax/train.py -s 142 -f configs/image/mnist/np.yaml    -lr 0.002811706625952 --datasets.train.batch_size 128 --train.num_epochs 20 --model.kwargs.loss_type ml
python scripts/jax/train.py -s 142 -f configs/image/mnist/bnp.yaml   -lr 0.002811706625952 --datasets.train.batch_size 128 --train.num_epochs 20
python scripts/jax/train.py -s 142 -f configs/image/mnist/canp.yaml  -lr 0.002811706625952 --datasets.train.batch_size 128 --train.num_epochs 20
python scripts/jax/train.py -s 142 -f configs/image/mnist/anp.yaml   -lr 0.000500000000000 --datasets.train.batch_size 128 --train.num_epochs 20 --model.kwargs.loss_type ml
python scripts/jax/train.py -s 142 -f configs/image/mnist/banp.yaml  -lr 0.000889139705019 --datasets.train.batch_size 128 --train.num_epochs 20
